====
CRF - 17.09.2019
====

Summary results: 
Worst fold

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            97.62        87.85        81.03        84.3         116    
<software>           89.65        87.77        71.93        79.06        399    
<url>                99.05        66.67        35.29        46.15        17     
<version>            97.75        88.62        85.16        86.85        128    

all (micro avg.)     96.01        87.63        75.15        80.91        660    
all (macro avg.)     96.01        82.73        68.35        74.09        660    

===== Instance-level results =====

Total expected instances:   231
Correct instances:          133
Instance-level recall:      57.58

Best fold:

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            97.06        81.63        75.47        78.43        106    
<software>           90.38        88           75.12        81.05        410    
<url>                99.53        85.71        70.59        77.42        17     
<version>            97.93        93.48        85.43        89.27        151    

all (micro avg.)     96.23        88.17        77.34        82.4         684    
all (macro avg.)     96.23        87.21        76.65        81.54        684    

===== Instance-level results =====

Total expected instances:   231
Correct instances:          131
Instance-level recall:      56.71


Average over 10 folds: 

label                accuracy     precision    recall       f1           support

<creator>            97.17        85.45        74.84        79.72        1120   
<software>           89.31        86.5         72.24        78.67        4130   
<url>                99.27        69.19        63.35        65.03        171    
<version>            97.88        89.65        84.99        87.14        1274   

all (macro avg.)     95.91        82.7         73.85        77.64  

===== Instance-level results =====

Total expected instances:   231.7
Correct instances:          126.1
Instance-level recall:      54.42






---------------
28.09.2019
-------------------------------
BidLSTM-CRF Gloves embeddings
-------------------------------

average over 10 folds
              precision    recall  f1-score   support

  <software>     0.7970    0.7521    0.7737       428
   <creator>     0.7757    0.8248    0.7994       105
   <version>     0.8855    0.9057    0.8955       140
       <url>     0.2822    0.3600    0.3136        15

    macro f1 = 0.7909
    macro precision = 0.7962
    macro recall = 0.7859 


** Worst ** model scores -
                  precision    recall  f1-score   support

      <software>     0.7855    0.7103    0.7460       428
       <version>     0.8811    0.9000    0.8905       140
       <creator>     0.7589    0.8095    0.7834       105
           <url>     0.1786    0.3333    0.2326        15

all (micro avg.)     0.7761    0.7558    0.7658       688


** Best ** model scores -
                  precision    recall  f1-score   support

      <software>     0.8173    0.7734    0.7947       428
       <version>     0.8889    0.9143    0.9014       140
       <creator>     0.8108    0.8571    0.8333       105
           <url>     0.6000    0.6000    0.6000        15

all (micro avg.)     0.8267    0.8110    0.8188       688







Sept. 23th 2019
-------------------------------------
BidLSTM-CRF Gloves embeddings + ELMo
-------------------------------------


average over 10 folds
              precision    recall  f1-score   support

  <software>     0.8687    0.8072    0.8363       428
   <creator>     0.8640    0.8781    0.8707       105
   <version>     0.8961    0.8907    0.8933       140
       <url>     0.6138    0.6400    0.6219        15

    macro f1 = 0.8487
    macro precision = 0.8672
    macro recall = 0.8314 


** Worst ** model scores -
                  precision    recall  f1-score   support

       <creator>     0.8142    0.8762    0.8440       105
      <software>     0.8233    0.8271    0.8252       428
           <url>     0.5789    0.7333    0.6471        15
       <version>     0.8971    0.8714    0.8841       140

all (micro avg.)     0.8295    0.8416    0.8355       688


** Best ** model scores -
                  precision    recall  f1-score   support

       <creator>     0.9057    0.9143    0.9100       105
      <software>     0.8675    0.8411    0.8541       428
           <url>     0.6923    0.6000    0.6429        15
       <version>     0.8794    0.8857    0.8826       140

all (micro avg.)     0.8726    0.8561    0.8643       688



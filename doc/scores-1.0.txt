====
CRF - 17.09.2019
====

Summary results: 
Worst fold

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            97.62        87.85        81.03        84.3         116    
<software>           89.65        87.77        71.93        79.06        399    
<url>                99.05        66.67        35.29        46.15        17     
<version>            97.75        88.62        85.16        86.85        128    

all (micro avg.)     96.01        87.63        75.15        80.91        660    
all (macro avg.)     96.01        82.73        68.35        74.09        660    

===== Instance-level results =====

Total expected instances:   231
Correct instances:          133
Instance-level recall:      57.58

Best fold:

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            97.06        81.63        75.47        78.43        106    
<software>           90.38        88           75.12        81.05        410    
<url>                99.53        85.71        70.59        77.42        17     
<version>            97.93        93.48        85.43        89.27        151    

all (micro avg.)     96.23        88.17        77.34        82.4         684    
all (macro avg.)     96.23        87.21        76.65        81.54        684    

===== Instance-level results =====

Total expected instances:   231
Correct instances:          131
Instance-level recall:      56.71


Average over 10 folds: 

label                accuracy     precision    recall       f1           support

<creator>            97.17        85.45        74.84        79.72        1120   
<software>           89.31        86.5         72.24        78.67        4130   
<url>                99.27        69.19        63.35        65.03        171    
<version>            97.88        89.65        84.99        87.14        1274   

all (macro avg.)     95.91        82.7         73.85        77.64  

===== Instance-level results =====

Total expected instances:   231.7
Correct instances:          126.1
Instance-level recall:      54.42


========
Runtime
========

threads   tokens/s
1         23684.4565
2         43280.7275
3         59866.8830
4         73338.9951
6         92385.1512
7         97658.6271
8         100878.4796


---------------
28.09.2019
-------------------------------
BidLSTM-CRF Gloves embeddings
-------------------------------

average over 10 folds
              precision    recall  f1-score   support

  <software>     0.7970    0.7521    0.7737       428
   <creator>     0.7757    0.8248    0.7994       105
   <version>     0.8855    0.9057    0.8955       140
       <url>     0.2822    0.3600    0.3136        15

    macro f1 = 0.7909
    macro precision = 0.7962
    macro recall = 0.7859 


** Worst ** model scores -
                  precision    recall  f1-score   support

      <software>     0.7855    0.7103    0.7460       428
       <version>     0.8811    0.9000    0.8905       140
       <creator>     0.7589    0.8095    0.7834       105
           <url>     0.1786    0.3333    0.2326        15

all (micro avg.)     0.7761    0.7558    0.7658       688


** Best ** model scores -
                  precision    recall  f1-score   support

      <software>     0.8173    0.7734    0.7947       428
       <version>     0.8889    0.9143    0.9014       140
       <creator>     0.8108    0.8571    0.8333       105
           <url>     0.6000    0.6000    0.6000        15

all (micro avg.)     0.8267    0.8110    0.8188       688


========
Runtime
========

tokens/s: 24774.3699 (batch size = 50)
tokens/s: 28706.5534 (batch size = 100)
tokens/s: 30247.0634 (batch size = 150)
tokens/s: 30519.4602 (batch size = 200)




Sept. 23th 2019
-------------------------------------
BidLSTM-CRF Gloves embeddings + ELMo
-------------------------------------


average over 10 folds
              precision    recall  f1-score   support

  <software>     0.8687    0.8072    0.8363       428
   <creator>     0.8640    0.8781    0.8707       105
   <version>     0.8961    0.8907    0.8933       140
       <url>     0.6138    0.6400    0.6219        15

    macro f1 = 0.8487
    macro precision = 0.8672
    macro recall = 0.8314 


** Worst ** model scores -
                  precision    recall  f1-score   support

       <creator>     0.8142    0.8762    0.8440       105
      <software>     0.8233    0.8271    0.8252       428
           <url>     0.5789    0.7333    0.6471        15
       <version>     0.8971    0.8714    0.8841       140

all (micro avg.)     0.8295    0.8416    0.8355       688


** Best ** model scores -
                  precision    recall  f1-score   support

       <creator>     0.9057    0.9143    0.9100       105
      <software>     0.8675    0.8411    0.8541       428
           <url>     0.6923    0.6000    0.6429        15
       <version>     0.8794    0.8857    0.8826       140

all (micro avg.)     0.8726    0.8561    0.8643       688


========
Runtime
========

tokens/s: 270.8676 (batch size = 5)




Jan. 9th 2020
-------------------------------------
bert-base en (cased)
-------------------------------------

sequence length 512 strict
batch size 6

** Worst ** model scores - run 4
                  precision    recall  f1-score   support

       <creator>     0.3796    0.3905    0.3850       105
      <software>     0.4860    0.4463    0.4653       428
           <url>     0.1304    0.2000    0.1579        15
       <version>     0.4714    0.4714    0.4714       140

all (micro avg.)     0.4533    0.4375    0.4453       688


** Best ** model scores - run 9
                  precision    recall  f1-score   support

       <creator>     0.7282    0.7143    0.7212       105
      <software>     0.8069    0.7617    0.7837       428
           <url>     0.5714    0.5333    0.5517        15
       <version>     0.8429    0.8429    0.8429       140

all (micro avg.)     0.7973    0.7660    0.7813       688

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

       <creator>     0.7293    0.7057    0.7172       105
      <software>     0.7558    0.7164    0.7355       428
           <url>     0.3870    0.5667    0.4550        15
       <version>     0.7854    0.7914    0.7883       140

all (macro avg.)     0.7448    0.7267    0.7356





Jan. 9th 2020
-------------------------------------
SciBERT
-------------------------------------

sequence length 512 strict
batch size 6

** Worst ** model scores - run 1
                  precision    recall  f1-score   support

       <creator>     0.8039    0.7810    0.7923       105
      <software>     0.8249    0.8037    0.8142       428
           <url>     0.6111    0.7333    0.6667        15
       <version>     0.8841    0.8714    0.8777       140

all (micro avg.)     0.8281    0.8125    0.8202       688


** Best ** model scores - run 5
                  precision    recall  f1-score   support

       <creator>     0.8367    0.7810    0.8079       105
      <software>     0.8623    0.8341    0.8480       428
           <url>     0.6667    0.8000    0.7273        15
       <version>     0.9254    0.8857    0.9051       140

all (micro avg.)     0.8660    0.8358    0.8506       688

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

       <creator>     0.7951    0.7771    0.7859       105
      <software>     0.8485    0.8243    0.8362       428
           <url>     0.6362    0.7533    0.6877        15
       <version>     0.8998    0.8800    0.8897       140

all (macro avg.)     0.8442    0.8269    0.8354 

-------------



Scores against **complete holdout set** (realistic in term of positive/negative class distribution)


* CRF - trained on only positive paragraphs (no negative sampling) 
2021-05-05

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            94.67        41.45        76.56        53.78        209    
<software>           76.94        29.18        58.49        38.93        648    
<url>                97.69        18.18        68.57        28.74        35     
<version>            95.79        51.85        84.85        64.37        231    

all (micro avg.)     91.27        34.58        67.59        45.75        1123   
all (macro avg.)     91.27        35.17        72.12        46.46        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32293
Instance-level recall:      96.64



* CRF - trained with random negative paragraphs (random oversampling) ratio 1

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.48        53.16        76.56        62.75        209    
<software>           78.02        36.49        57.72        44.71        648    
<url>                98.31        28.57        68.57        40.34        35     
<version>            96.55        63.87        85.71        73.2         231    

all (micro avg.)     92.09        43.95        67.32        53.18        1123   
all (macro avg.)     92.09        45.52        72.14        55.25        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32664
Instance-level recall:      97.75



* CRF - trained with random negative paragraphs (random oversampling) ratio 5

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.23        57.3         77.03        65.71        209    
<software>           80.99        48.54        56.48        52.21        648    
<url>                98.5         34.48        57.14        43.01        35     
<version>            96.74        70.86        85.28        77.41        231    

all (micro avg.)     92.86        54.27        66.25        59.66        1123   
all (macro avg.)     92.86        52.8         68.98        59.59        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32820
Instance-level recall:      98.22



* CRF - trained with random negative paragraphs (random oversampling) ratio 10

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.89        67.67        75.12        71.2         209    
<software>           82.93        60.13        55.4         57.67        648    
<url>                98.35        35.71        57.14        43.96        35     
<version>            96.73        74.62        85.28        79.6         231    

all (micro avg.)     93.47        63.79        65.27        64.52        1123   
all (macro avg.)     93.47        59.54        68.24        63.11        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          32968
Instance-level recall:      98.66


* CRF - trained with random negative paragraphs (random oversampling) ratio 15

===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.93        70.4         75.12        72.69        209    
<software>           83.7         66.92        53.7         59.59        648    
<url>                98.31        34.78        45.71        39.51        35     
<version>            97           79.75        83.55        81.61        231    

all (micro avg.)     93.73        69.25        63.58        66.3         1123   
all (macro avg.)     93.73        62.97        64.52        63.35        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          33024
Instance-level recall:      98.83


* CRF - trained with model-selected negative paragraphs (active oversampling) 


===== Field-level results =====

label                accuracy     precision    recall       f1           support

<creator>            95.8         70.32        73.68        71.96        209    
<software>           83.9         68.95        52.78        59.79        648    
<url>                98.22        32.61        42.86        37.04        35     
<version>            97.03        80.93        82.68        81.8         231    

all (micro avg.)     93.74        70.41        62.51        66.23        1123   
all (macro avg.)     93.74        63.2         63           62.65        1123   

===== Instance-level results =====

Total expected instances:   33416
Correct instances:          33037
Instance-level recall:      98.87



* bidLSTM-CRF - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
    f1 (micro): 41.88
                  precision    recall  f1-score   support

       <creator>     0.4529    0.8278    0.5854       209
      <software>     0.2194    0.6852    0.3323       648
           <url>     0.1667    0.5714    0.2581        35
       <version>     0.5359    0.9048    0.6731       231

all (micro avg.)     0.2901    0.7533    0.4189      1123

Evaluation runtime: 350.445 seconds


* bidLSTM-CRF - random sampling ratio 15 (best ratio), eval complete holdout
2021-05-05

Evaluation:
        f1 (micro): 69.03
                  precision    recall  f1-score   support

       <creator>     0.6742    0.8517    0.7526       209
      <software>     0.5711    0.7191    0.6366       648
           <url>     0.5098    0.7429    0.6047        35
       <version>     0.7295    0.8874    0.8008       231

all (micro avg.)     0.6197    0.7792    0.6903      1123



* bidLSTM-CRF - active sampling, eval complete holdout
2021-05-05

Evaluation:
    f1 (micro): 69.81
                  precision    recall  f1-score   support

       <creator>     0.6899    0.8517    0.7623       209
      <software>     0.6271    0.6852    0.6549       648
           <url>     0.6316    0.6857    0.6575        35
       <version>     0.6350    0.9264    0.7535       231

all (micro avg.)     0.6413    0.7658    0.6981      1123

Evaluation runtime: 343.414 seconds 


* bidLSTM-CRF_FEATURES - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
        f1 (micro): 41.42
                  precision    recall  f1-score   support

       <creator>     0.4566    0.8565    0.5957       209
      <software>     0.2094    0.7454    0.3269       648
           <url>     0.1453    0.4857    0.2237        35
       <version>     0.5840    0.9177    0.7138       231

all (micro avg.)     0.2803    0.7934    0.4142      1123

Evaluation runtime: 389.84 seconds


* bidLSTM-CRF_FEATURES - random sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
    f1 (micro): 68.31
                  precision    recall  f1-score   support

       <creator>     0.6848    0.8421    0.7554       209
      <software>     0.5408    0.7361    0.6235       648
           <url>     0.5000    0.6571    0.5679        35
       <version>     0.7220    0.9221    0.8099       231

all (micro avg.)     0.6007    0.7916    0.6831      1123

Evaluation runtime: 438.95 seconds


* bidLSTM-CRF_FEATURES - active sampling (best ratio)
2021-05-05

Evaluation:
    f1 (micro): 69.25
                  precision    recall  f1-score   support

       <creator>     0.6820    0.8517    0.7574       209
      <software>     0.5454    0.7330    0.6254       648
           <url>     0.4746    0.8000    0.5957        35
       <version>     0.7948    0.9221    0.8537       231

all (micro avg.)     0.6127    0.7961    0.6925      1123

Evaluation runtime: 443.199 seconds 




* BERT-base-en - trained on only positive paragraphs (no negative sampling) , eval complete holdout
2021-05-05

Evaluation:
                  precision    recall  f1-score   support

       <creator>     0.4019    0.7943    0.5338       209
      <software>     0.1508    0.7423    0.2507       648
           <url>     0.0449    0.7143    0.0845        35
       <version>     0.4212    0.8788    0.5694       231

all (micro avg.)     0.1885    0.7792    0.3036      1123

Evaluation runtime: 1815.756 seconds 


* BERT-base-en - random sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 1237
                  precision    recall  f1-score   support

       <creator>     0.6157    0.7895    0.6918       209
      <software>     0.5276    0.6775    0.5932       648
           <url>     0.1496    0.5429    0.2346        35
       <version>     0.6589    0.8528    0.7434       231

all (micro avg.)     0.5374    0.7302    0.6191      1123

Evaluation runtime: 1812.012 seconds 


* BERT-base-en - active sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 1254
                  precision    recall  f1-score   support

       <creator>     0.6613    0.7847    0.7177       209
      <software>     0.5685    0.6790    0.6188       648
           <url>     0.1900    0.5429    0.2815        35
       <version>     0.7351    0.8528    0.7896       231

all (micro avg.)     0.5899    0.7302    0.6526      1123

Evaluation runtime: 1810.536 seconds 




* Bid-LSTM-CRF+ELMo - trained on only positive paragraphs (no negative sampling) 
2021-05-05


Evaluation:
    f1 (micro): 54.51
                  precision    recall  f1-score   support

       <creator>     0.7155    0.7943    0.7528       209
      <software>     0.3556    0.7485    0.4821       648
           <url>     0.1162    0.8000    0.2029        35
       <version>     0.7286    0.8831    0.7984       231

all (micro avg.)     0.4171    0.7863    0.5451      1123


* Bid-LSTM-CRF+ELMo - random sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
        f1 (micro): 70.16
                  precision    recall  f1-score   support

       <creator>     0.6387    0.8373    0.7246       209
      <software>     0.6744    0.6296    0.6512       648
           <url>     0.5484    0.4857    0.5152        35
       <version>     0.8305    0.8485    0.8394       231

all (micro avg.)     0.6946    0.7088    0.7016      1123

Evaluation runtime: 19858.772 seconds 



* Bid-LSTM-CRF+ELMo - active sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
    f1 (micro): 71.63
                  precision    recall  f1-score   support

       <creator>     0.7406    0.8469    0.7902       209
      <software>     0.6187    0.7037    0.6585       648
           <url>     0.4800    0.6857    0.5647        35
       <version>     0.7770    0.9048    0.8360       231

all (micro avg.)     0.6687    0.7711    0.7163      1123

Evaluation runtime: 20004.177 seconds



* SciBERT-CRF - trained on only positive paragraphs (no negative sampling) 
2021-05-05

Evaluation:
number of alignment issues with test set: 995
                  precision    recall  f1-score   support

       <creator>     0.4414    0.8469    0.5803       209
      <software>     0.2573    0.8040    0.3898       648
           <url>     0.2778    0.7143    0.4000        35
       <version>     0.7172    0.9221    0.8068       231

all (micro avg.)     0.3327    0.8335    0.4756      1123

Evaluation runtime: 1794.844 seconds 


* SciBERT-CRF - random sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 990
                  precision    recall  f1-score   support

       <creator>     0.6811    0.8278    0.7473       209
      <software>     0.6048    0.7701    0.6775       648
           <url>     0.4032    0.7143    0.5155        35
       <version>     0.7536    0.9134    0.8258       231

all (micro avg.)     0.6390    0.8085    0.7138      1123

Evaluation runtime: 1801.454 seconds 


* SciBERT-CRF - active sampling ratio 15 (best ratio)
2021-05-05

Evaluation:
number of alignment issues with test set: 999
                  precision    recall  f1-score   support

       <creator>     0.7555    0.8278    0.7900       209
      <software>     0.6931    0.7284    0.7103       648
           <url>     0.4528    0.6857    0.5455        35
       <version>     0.8024    0.8788    0.8388       231

all (micro avg.)     0.7171    0.7765    0.7456      1123

Evaluation runtime: 1790.766 seconds 



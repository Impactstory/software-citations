<tei xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc xml:id="PMC4898791" /><encodingDesc><appInfo><application version="0.5.6-SNAPSHOT" ident="GROBID" when="2019-06-07T17:21+0000"><ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref></application></appInfo></encodingDesc></teiHeader>
<text xml:lang="en">
<p>Abstract </p>

<p>We present an efficient and flexible method for computing likelihoods for phenotypic traits on a phylogeny. The method does 
not resort to Monte Carlo computation but instead blends Felsenstein's discrete character pruning algorithm with methods for 
numerical quadrature. It is not limited to Gaussian models and adapts readily to model uncertainty in the observed trait values. 
We demonstrate the framework by developing efficient algorithms for likelihood calculation and ancestral state reconstruction under 
Wright's threshold model, applying our methods to a data set of trait data for extrafloral nectaries across a phylogeny of 839 Fabales 
species. </p>

<p>Statistical models for nucleotide or amino acid mutations and 
substitutions, and the algorithms for computing with them, 
are fundamental to the study of molecular evolution and 
biology. As we widen our focus from the evolution of genes 
to the evolution of genomes, individuals, and populations, a 
whole new class of modeling challenges present themselves. 
These include the development of realistic quantitative models 
for traits which vary over a continuous range of values 
(O'Meara 2012). Of course, the usefulness of any new 
model is contingent on the tools available to compute with 
them. The main contribution of this article is to show how, by 
combining ideas from statistical phylogenetics and numerical 
mathematics, we can compute efficiently with a far larger 
range of evolutionary models. 
The algorithms we develop are for computation of the like-
lihood, that is the probability of the data given the phylogeny, 
evolutionary model and parameters. If we are working with an 
evolutionary model with only a small (finite) number of states, 
then likelihoods can be computed using the dynamic pro-
gramming algorithm of Felsenstein (1981a). We will show 
how to extend this algorithm to also compute likelihoods for 
(essentially) arbitrary continuous trait models. 
There is already a wide range of evolutionary phenomena 
that are studied using continuous trait models. Much of </p>

<p>comparative genomics relies on implicit or explicit models 
for the evolution of morphology (Stevens 1991; Felsenstein 
2002; Ronquist 2004; Harmon et al. 2010; O'Meara 2012), 
many of which make gross simplifying assumptions about 
how traits vary over time. Continuous evolutionary models 
have been used in comparative transcriptomics to study her-
itable aspects of gene expression levels (Khaitovich et al. 2005, 
2006), an area with exceptional promise given recent im-
provements in accuracy and the ability to sample in situ 
(Voelckel et al. 2002). 
Continuous trait models will be of growing importance in 
evolutionary studies of whole-genome single nucleotide poly-
morphism-databases. Inference methods based on the coales-
cent such as <rs id="software-1" type="software">SNAPP</rs> (<rs corresp="#software-1" type="creator">Bryant et al.</rs>. 2012) do not scale well as 
the number of individuals grows, while those based on con-
tinuous models of gene frequencies (Cavalli-Sforza and 
Edwards 1967; Felsenstein 1981b; Siré n et al. 2011) depend 
only on proportions of populations with each allele, so scale 
extremely well. In addition, it is often easier to model the 
effect of selection on continuous gene frequency models 
than with the coalescent. Continuous evolutionary models 
have also been applied successfully to the study of ancestral 
geography distributions (Lemey et al. 2010). 
Our interest is in developing techniques used to compute 
with these models, and to expand the range of models we can </p>

<p>GBE </p>

<p>ß The Author(s) 2016. Published by Oxford University Press on behalf of the Society for Molecular Biology and Evolution. 
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits 
non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com </p>

<p>1338 Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>

<p>work with. Early work of Felsenstein (1968, 1973), revisited by 
Freckleton (2012) and FitzJohn (2012), demonstrated that if 
traits are evolving according to Brownian motion then we can 
compute likelihoods quickly and (up to numerical precision) 
exactly. Felsenstein's approach extends to other Gaussian pro-
cesses, notably the Ornstein-Uhlenbeck (OU) process (Lande 
1976; Felsenstein 1988; Hansen 1997), and for several dec-
ades, Gaussian models were used almost exclusively to model 
the evolution of quantitative traits. Ho and Ané (2014) used 
clever algebraic techniques to develop an alternative algorithm 
for computing the likelihood and related quantities. They 
survey several other models which can be handled using the 
same approach. 
These methods are very efficient, and when they can be 
used, they should be used. The drawback of these methods is 
that they are fundamentally restricted to models which are 
Gaussian processes or transforms of Gaussian processes, 
where the computational bottleneck lies in the computation 
of a quadratic form involving the covariance matrix of Ho and 
Ané (2014). Many evolutionary models cannot be handled 
within this framework (e.g., Ronquist 2004; Landis et al. 
2013). Some of the properties of Gaussian processes are 
quite restrictive: Gaussian processes have single modes, so 
can only model adaptive landscapes with single peaks; 
Brownian motion has independent increments, so the rate 
of change is independent of the value of a trait. The standard 
strategy for computing with non-Gaussian models is to resort 
to Monte-Carlo strategies. Even when we are working with a 
model satisfying the assumptions of Ho and Ané (2014), the 
algorithms they describe do not give an efficient method for 
integrating over sets of trait values at the tips, as in the thresh-
old models we discuss below. 
Computing the probability of quantitative character evolu-
tion may be framed as a numerical integration (quadrature) 
problem. For most models, if we know the value of the trait at 
each ancestral node in the phylogeny, we can quickly com-
pute the various transition probabilities. Because we do not 
usually know these ancestral trait values we integrate them 
out. This is a multidimensional integration problem with one 
dimension for each ancestral node (or two dimensions for 
each node if we are modeling covarying traits) see 
Felsenstein (2004). 
Methods for estimating or approximating integrals are usu-
ally judged by their "rate of convergence": how quickly the 
error of approximation decreases as the amount of work 
(function evaluations) increases. Consider the problem of 
computing a one-dimensional integral 
Z 1 </p>

<p>0 </p>

<p>f ðxÞdx 
ð1Þ </p>

<p>where f is a "nice" function with continuous and bounded 
derivatives. Simpson's rule, a simple textbook method re-
viewed below, can be shown to have an OðN 
À4 Þ rate of </p>

<p>convergence, meaning that, asymptotically in N, evaluating 
ten times more points reduces the error by a factor of 10 </p>

<p>4 </p>

<p>. 
In contrast, a standard Monte Carlo method has a rate of 
convergence of OðN </p>

<p>À </p>

<p>1 </p>

<p>2 Þ, meaning that evaluating ten times 
more points will only reduce the error by a factor of around 
3. For this reason, numerical analysis texts often refer to 
Monte Carlo approaches as "methods of last resort." 
Despite this apparently lacklustre performance guarantee, 
Monte Carlo methods have revolutionized phylogenetics in 
general and the analysis of quantitative characters in particu-
lar. The reason is their partial immunity to the curse of dimen-
sionality. Methods like Simpson's rule are not practical for a 
high number of dimensions as the asymptotic convergence 
rate, quoted above, is only achieved for an infeasibly large 
number of function evaluations N. The effective convergence 
rate for small N can be very poor, and typically worse than 
Monte Carlo. In contrast, there are Monte Carlo approaches 
which achieve close to OðN </p>

<p>À </p>

<p>1 </p>

<p>2 Þ convergence irrespective of 
dimension. This has been critical when computing the likeli-
hoods of complex evolutionary models with as many dimen-
sions as there are nodes in the phylogeny. 
The main contribution of our article is to demonstrate how 
to efficiently and accurately compute likelihoods on a phylog-
eny using a sequence of one-dimensional integrations. We 
obtain a fast algorithm with convergence guarantees that 
far exceed what can be obtained by Monte Carlo integration. 
Our approach combines two standard tools: classical numer-
ical integrators and Felsenstein's pruning algorithm for dis-
crete characters (Felsenstein 1981a). Indeed, the only real 
difference between our approach and Felsenstein's discrete 
character algorithm is that we use numerical integration tech-
niques to integrate states at ancestral nodes, instead of just 
carrying out a summation. 
The running time of the algorithm is OðN 
2 nÞ, where N is 
the number of points used in the numerical integration at 
each node and n is the number of taxa (leaves) in the tree. 
Using Simpson's method, we obtain a convergence rate of 
OðnN 
À4 Þ, meaning that if we increase N by a factor of 10, we 
will obtain an estimate which is accurate to four more decimal 
places. 
To illustrate the application of our general framework, we 
develop an efficient algorithm for computing the likelihood 
of a tree under the threshold model of Wright (1934) 
and Felsenstein (2005, 2012). We also show how to infer 
marginal trait densities at ancestral nodes. We have imple-
mented these algorithms and used them to study evolution 
of extrafloral nectaries (EFN) on an 839-taxon phylogeny of 
Marazzi et al. (2012). <rs type="software">MATLAB</rs> code for computing the 
threshold likelihood has been posted on <rs type="software">MATLAB</rs> Central 
and complete <rs type="software">MATLAB</rs> code for all analyses and simulations 
can be found in supplementary material, Supplementary 
Material online. 
The combination of numerical integrators and the pruning 
algorithm opens up a large range of potential models and </p>

<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>



<p>approaches which we have only just begun to explore. It may 
well be that Gaussian type models provide good approxima-
tions in many contexts, however the extent to which this is 
true will be unknown until we have computational tools for 
handling richer models. </p>

<p>Materials and Methods </p>

<p>Models for Continuous Trait Evolution </p>

<p>Phylogenetic models for continuous trait evolution, like those 
for discrete traits, are specified by the density of trait values at 
the root and the transition densities along the branches. We 
use f ðx r jy r Þ to denote the density for the trait value at the 
root, where r is a set of relevant model parameters. We use 
f ðx i jx j ; y i Þ to denote the transitional density for the value at 
node i, conditional on the trait value at its parent node j. Here, 
i represents a bundle of parameters related to node i such as 
branch length, population size, and mutation rate. All of these 
parameters can vary throughout the tree. 
To see how the model works, consider how continuous 
traits might be simulated. A state X r is sampled from the 
root density f ðX r jy r Þ. We now proceed through the phylogeny 
from the root to the tips, each time visiting a node only after 
its parent has already been visited. For each node i, we gen-
erate the value at that node from the density f ðX i jx j ; y v Þ, 
where x j is the simulated trait value at node j, the parent of 
node i. In this way, we will eventually generate trait values for 
the tips. 
We use X 1 , . . ., X n to denote the random trait values at 
the tips and X nþ1 ; . . . ; X 2nÀ1 to denote the random trait 
values at the internal nodes, ordered so that children 
come before parents. Hence, X 2nÀ1 is the state assigned to 
the root. Let </p>

<p>EðT Þ ¼ fði; jÞ : node i is a child of node jg 
ð2Þ </p>

<p>denote the set of branches in the tree. The joint density for all 
trait values, observed and ancestral, is given by multiplying the 
root density with all of the transition densities </p>

<p>f ðx 1 ; . . . ; x n ; x nþ1 ; . . . ; x 2nÀ1 jyÞ 
¼ f ðx 2nÀ1 jyÞ 
Y </p>

<p>ði;jÞ2EðT Þ </p>

<p>f ðx i jx j ; y i Þ: 
ð3Þ </p>

<p>The probability of the observed trait values x 1 ; . . . ; x n is 
now determined by integrating out all of the ancestral trait 
values: </p>

<p>LðTÞ ¼ f ðx 1 ; . . . ; x n jyÞ ¼ </p>

<p>Z Z 
Á Á Á </p>

<p>Z 
f ðx 2nÀ1 jy r Þ 
Y </p>

<p>ði;jÞ2EðTÞ </p>

<p>f ðx i jx j ; y i Þdx nþ1 ; . . . ; dx 2nÀ1 : 
ð4Þ </p>

<p>In these integrals, the bounds of integration will vary ac-
cording to the model. 
The oldest, and most widely used, continuous trait models 
assume that traits (or transformed gene frequencies) evolve 
like Brownian motion (Cavalli-Sforza and Edwards 1967; 
Felsenstein 1973). For these models, the root density f ðx r jyÞ 
is Gaussian (normal) with mean 0 and unknown variance s </p>

<p>2 </p>

<p>r . 
The transition densities f ðx i jx j ; y v Þ are also Gaussian, with 
mean x j (the trait value of the parent) and variance propor-
tional to branch length. Note that there are identifiability 
issues which arise with the inference of the root position 
under this model, necessitating a few tweaks in practice 
(see the discussion in Chapter 23 of Felsenstein 2004). 
It can be shown that when the root density and transitional 
densities are all Gaussian, the joint density (4) is multivariate 
Gaussian. Furthermore, the covariance matrix for this density 
has a special structure which methods such as the pruning 
techniques of Felsenstein (1968, 1973), Freckleton (2012), 
and FitzJohn (2012) exploit, as does the top-down approach 
of Ho and Ané (2014). This general approach continues to 
work when Brownian motion is replaced by an OU process 
(Lande 1976; Felsenstein 1988; Hansen 1997), or indeed to 
many linear or generalized linear models. 
Gaussian models, and their relatives, are mathematically 
and computationally convenient, but rely on assumptions 
which are unrealistic and inappropriate in many contexts. 
Numerous researchers have implemented models which do 
not fit into the general Gaussian framework; most have re-
sorted to Monte Carlo computation to carry out their analyses. 
Landis et al. (2013) discuss a class of continuous trait 
models which are based on Lé vy processes and include 
jumps. At particular times, as governed by a Poisson process, 
the trait value jumps to a value drawn from a given density. 
Examples include a compound Poisson process with Gaussian 
jumps and a Variance Gamma model given by Brownian 
motion with time varying according to a gamma process. 
Both of these processes have analytical transition probabilities 
in some special cases. 
Lepage et al. (2006) use the Cox-Ingersoll-Ross (CIR) pro-
cess to model rate variation across a phylogeny. Like the OU 
process (but unlike Brownian motion), the CIR process is er-
godic. It has a stationary Gamma density which can be used 
for the root density. The transition density is a particular 
noncentral chi-squared density and the process only assumes 
positive values. 
Kutsukake and Innan (2013) examine a family of com-
pound Poisson models, focusing particularly on a model 
where the trait values make exponentially distributed jumps 
upwards or downwards. In the case that the rates of upward 
and downward jumps are the same, the model has jumps that 
follow a double exponential distribution. Kutsukake and Innan </p>

<p>Hiscott et al. </p>

<p>GBE </p>



<p>(2013) use approximate Bayesian computation to carry out 
inference. 
Siré n et al. (2011) propose a simple and elegant model for 
gene frequencies whereby the root value is drawn from a Beta 
distribution and each transitional density is Beta with appro-
priately chosen parameters. 
Trait values at the tips are not always observed directly. A 
simple, but important, example of this is the threshold model 
of Wright (1934), explored by Felsenstein (2005). Under this 
model, the trait value itself is censored and we only observe 
whether or not the value is positive or negative. A similar 
complication arises when dealing with gene frequency data 
as we typically do not observe the actual gene frequency but 
instead a binomially distributed sample based on that fre-
quency (Siré n et al. 2011). 
If the trait values at the tip are not directly observed we 
integrate over these values as well. Let pðz i jx i Þ denote the 
probability of observing z i given the trait value x i . The margin-
alized likelihood is then </p>

<p>LðT jz 1 ; . . . ; z n Þ ¼ </p>

<p>Z Z 
Á Á Á </p>

<p>Z 
f ðx r jyÞ 
Y </p>

<p>ði;jÞ2EðT Þ </p>

<p>f ðx i jx j ; y v Þ 
Y n </p>

<p>i¼1 </p>

<p>pðz i jx i Þdx 1 ; . . . ; dx 2nÀ1 : </p>

<p>ð5Þ </p>

<p>Numerical Integration </p>

<p>Analytical integration can be difficult or impossible. For the 
most part, it is unusual for an integral to have an analytical 
solution and there is no general method for finding it when it 
does exist. In contrast, numerical integration techniques (also 
known as numerical quadrature) are remarkably effective and 
are often easy to implement. A numerical integration method 
computes an approximation of the integral from function 
values at a finite number of points. Hence, we can obtain 
approximate integrals of functions even when we do not 
have an equation for the function itself. See Cheney and 
Kincaid (2012) for an introduction to numerical integration, 
and Dahlquist and Bjö rck (2008) and Davis and Rabinowitz 
(1984) for more comprehensive technical surveys. 
The idea behind most numerical integration techniques is 
to approximate the target function using a function which is 
easy to integrate. In this article, we will restrict our attention to 
Simpson's method which approximates the original function 
using piecewise quadratic functions. To approximate an 
integral 
R b 
a f ðxÞdx we first determine N + 1 equally spaced 
points (N even) </p>

<p>x 0 ¼ a; x 1 ¼ a þ 
b À a 
N 
; x 2 ¼ a þ 2 
b À a 
N 
; . . . ; </p>

<p>x k ¼ a þ k 
b À a 
N 
; . . . ; x N ¼ b: </p>

<p>ð6Þ </p>

<p>We now divide the integration into N=2 intervals </p>

<p>Z b </p>

<p>a </p>

<p>f ðxÞdx ¼ 
X N=2 </p>

<p>'¼1 </p>

<p>Z x 2' </p>

<p>x 2'À2 </p>

<p>f ðxÞdx: 
ð7Þ </p>

<p>Within each interval ½x 2'À2 ; x 2' , there is a unique quadratic 
function which equals f(x) at each the three points 
x ¼ x 2'À2 ; x ¼ x 2'À1, and x ¼ x 2' . The integral of this qua-
dratic on the interval ½x 2'À2 ; x 2' is </p>

<p>ðb À aÞ 
3N 
f ðx 2'À2 Þ þ 4f ðx 2'À1 Þ þ f ðx 2' Þ 
ð 
Þ 
ð 8Þ </p>

<p>Summing over ', we obtain the approximation 
Z b </p>

<p>a </p>

<p>f ðxÞdx&amp; 
X N=2 </p>

<p>'¼1 </p>

<p>ðb À aÞ 
3N 
f ðx 2'À2 Þ þ 4f ðx 2'À1 Þ þ f ðx 2' Þ 
ð 
Þ : ð9Þ </p>

<p>With a little rearrangement, the approximation can be writ-
ten in the form 
Z b </p>

<p>a </p>

<p>f ðxÞdx&amp; 
ðb À aÞ 
N </p>

<p>X N </p>

<p>k¼0 </p>

<p>w k f ðx k Þ 
ð 10Þ </p>

<p>where w k ¼ 4=3 when k is odd and w k ¼ 2=3 when k 
is even, with the exception of w 0 and w N which both equal 
1/3. Simpson's method is easy to implement and has a con-
vergence rate of OðN 
À4 Þ. Increasing the number of intervals 
by a factor of 10 decreases the error by a factor of 10 
À4 . See 
Dahlquist and Bjö rck (2008) and Davis and Rabinowitz (1984) 
for further details. 
It should be remembered, however, that the convergence 
rate is still only an asymptotic bound, and gives no guarantees 
on how well the method performs for a specific function and 
choice of N. Simpson's method, for example, can perform 
quite poorly when the function being integrated has rapid 
changes or sharp peaks. We observed this behavior when 
implementing threshold models, as described below. Our re-
sponse was to better tailor the integration method for the 
functions appearing. We noted that the numerical integra-
tions we carried out all had the form 
Z b </p>

<p>a </p>

<p>e </p>

<p>À </p>

<p>ðxÀmÞ 2 </p>

<p>2s 2 f ðxÞdx 
ð11Þ </p>

<p>where and varied. Using the same general approach as 
Simpson's rule, we approximated f(x), rather than the whole </p>

<p>function e </p>

<p>À </p>

<p>ðxÀmÞ 2 </p>

<p>2s 2 f ðxÞ, by a piecewise quadratic function p(x). 
We could then use standard techniques and tools to evaluate 
R b 
a e </p>

<p>À </p>

<p>ðxÀmÞ 2 </p>

<p>2s 2 pðxÞdx numerically. The resulting integration for-
mula, which we call the "Gaussian kernel method," gives a 
significant improvement in numerical accuracy. 
A further complication is that, in models of continuous 
traits, the trait value often ranges over the whole real line, </p>

<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>



<p>or at least over the set of positive reals. Hence, we need to 
approximate integrals of the form 
Z 1 </p>

<p>À1 </p>

<p>f ðxÞdx or </p>

<p>Z 1 </p>

<p>0 </p>

<p>f ðxÞdx 
ð12Þ </p>

<p>though the methods discussed above only apply to integrals 
on finite intervals. We truncate these integrals, determining 
values U and L such that the difference 
Z 1 </p>

<p>À1 </p>

<p>f ðxÞdx À </p>

<p>Z U </p>

<p>L </p>

<p>f ðxÞdx 
ð13Þ </p>

<p>between the full integral 
R 1 
À1 f ðxÞdx and the truncated inte-
gral 
R U 
L f ðxÞdx can be bounded analytically. Other strategies 
are possible; see Dahlquist and Bjö rck (2008) for a compre-
hensive review. </p>

<p>A Pruning Algorithm for Integrating Continuous Traits </p>

<p>Felsenstein has developed pruning algorithms for both 
continuous and discrete characters (Felsenstein 1981a,b). His 
algorithm for continuous characters works only for Gaussian 
processes. Our approach is to take his algorithm for discrete 
characters and adapt it to continuous characters. 
The (discrete character) pruning algorithm is an application 
of dynamic programming. For each node i, and each state x, 
we compute the probability of observing the states for all tips 
which are descendants of node i, conditional on node i having 
ancestral state x. This probability is called the partial likelihood 
at node i given state x. Our algorithm follows the same 
scheme, with one major difference. Since traits are continu-
ous, we cannot store all possible partial likelihoods. Instead, 
we store likelihoods for a finite set of values and plug these 
values into a numerical integration routine. 
Let i be the index of a node in the tree not equal to the root, 
let node j be its parent node. We define the partial likelihood, 
F i ðx j Þ; to be the likelihood for the observed trait values at the 
tips which are descendants of node i, conditional on the 
parent node j having trait value x j . If node i is a tip with ob-
served trait value x i we have </p>

<p>F i ðx j Þ ¼ f ðx i jx j ; y i Þ 
ð 14Þ </p>

<p>recalling that f ðx i jx j ; y i Þ is the density for the value of the 
trait at node i conditional on the value of the trait for its 
parent. More generally, we may only observe some value z i 
for which we have the conditional probability pðz i jx i Þ condi-
tional on the trait value x i . In this case, the partial likelihood is 
given by </p>

<p>F i ðx j Þ ¼ </p>

<p>Z 
f ðx i jx j ; y i Þpðz i jx i Þdx i : 
ð15Þ </p>

<p>Suppose node i is not the root and that it has two children u 
and v. Since trait evolution is conditionally independent on 
disjoint subtrees, we obtain the recursive formula </p>

<p>F i ðx j Þ ¼ </p>

<p>Z 
f ðx i jx j ; y i ÞF u ðx i ÞF v ðx i Þdx i : 
ð16Þ </p>

<p>Finally, suppose that node i is the root and has two children 
u and v. We evaluate the complete tree likelihood using the 
density of the trait value at the root, </p>

<p>LðT Þ ¼ </p>

<p>Z 
f ðxjy r ÞF u ðxÞF v ðxÞdx: 
ð17Þ </p>

<p>The bounds of integration in (15)-(17) will vary according 
to the model. 
We use numerical integration techniques to approximate 
(15)-(17) and dynamic programming to avoid an exponential 
explosion in the computation time. Let N denote the number 
of function evaluations for each node. In practice, this might 
vary over the tree, but for simplicity we assume that it is con-
stant. For each node i, we select N + 1 trait values </p>

<p>X i ½0 &lt; X i ½1 &lt; Á Á Á &lt; X i ½N: 
ð18Þ </p>

<p>How we do this will depend on the trait model and the 
numerical integration technique. If, for example, the trait 
values vary between a and b and we are applying Simpson's 
method with N intervals we would use X i ½k ¼ a þ </p>

<p>bÀa </p>

<p>N k for 
k ¼ 0; 1; 2; . . . ; N. 
We traverse the tree starting at the tips and working to-
ward the root. For each nonroot node i and k ¼ 0; 1; . . . ; N 
we compute and store an approximation F i ½k of F i ðX j ½kÞ, 
where node j is the parent of node i. Note that this is an ap-
proximation of F i ðX j ½kÞ rather than of F i ðX i ½kÞ since F i ðxÞ is 
the partial likelihood conditional on the trait value for the 
parent of node i. The value approximation F v ½i is computed 
by applying the numerical integration method to the appro-
priate integral (15)-(17), where we replace function evalua-
tions with approximations previously computed. See below 
for a worked example of this general approach. 
The numerical integration methods we use run in time linear 
in the number of points being evaluated. Hence, if n is the 
number of tips in the tree, the algorithm will run in time 
OðnN 
2 Þ. For the integration techniques described above, the 
convergence rate (in N) for the likelihood on the entire tree had 
the same order as the convergence rate for the individual one-
dimensional integrations (see below for a formal proof of a 
specific model). We have therefore avoided the computational 
blow-out typically associated with such high-dimensional inte-
grations, and achieve this without sacrificing accuracy. </p>

<p>Posterior Densities for Ancestral States </p>

<p>The algorithms we have described compute the joint density 
of the states at the tips, given the tree, the branch lengths, and 
other parameters. As with discrete traits, the algorithms can 
be modified to infer ancestral states for internal nodes in the 
tree. Here, we show how to carry out reconstruction of </p>

<p>Hiscott et al. </p>

<p>GBE </p>

<p>1342 Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>

<p>the marginal posterior density of a state at a particular node. 
The differences between marginal and joint reconstructions 
are reviewed in Yang (2006, p. 121). 
First consider marginal reconstruction of ancestral states at 
the root. Let u and v be the children of the root. The product 
F u ðxÞF v ðxÞ equals the probability of the observed character 
conditional on the tree, branch lengths, parameters, and a 
state of x at the root. The marginal probability of x, ignoring 
the data, is given by the root density f ðxjy r Þ. Integrating the 
product of F u ðxÞF v ðxÞ and f ðxjy r Þ gives the likelihood LðT Þ, 
as in (17). Plugging these into Bayes' rule, we obtain the pos-
terior density of the state at the root: </p>

<p>f ðx r jz 1 ; . . . ; z n Þ ¼ 
F u ðx r ÞF v ðx r Þf ðx r jy r Þ 
LðT Þ 
: 
ð19Þ </p>

<p>With general time reversible models used in phylogenetics, 
the posterior distributions at other nodes can be found by 
changing the root of the tree. Unfortunately, the same 
trick does not work for many quantitative trait models. 
Furthermore, recomputing likelihoods for each possible root 
entails a large amount of unnecessary computation. 
Instead, we derive a second recursion, this one starting at 
the root and working toward the tips. A similar trick is used to 
compute derivatives of the likelihood function in Felsenstein 
and Churchill (1996). For a node i and state x we let G i ðxÞ 
denote the likelihood for the trait values at tips which are not 
descendants of node i, conditional on node i having trait value 
x. If node i is the root r, then G r ðxÞ is 1 for all x. 
Let node i be any node apart from the root, let node j be its 
parent and let node u be the other child of j (that is, the sibling 
of node i). We letx denote the trait value at node j. Then G i ðxÞ 
can be written </p>

<p>G i ðxÞ ¼ </p>

<p>Z 
f ðxjx; y i ÞG j ðxÞF u ðxÞdx: 
ð20Þ </p>

<p>This integral can be evaluated using the same numerical 
integrators used when computing likelihoods. Note that f ðxj 
x; y i Þ is the conditional density of the parent state given the 
child state, which is the reverse of the transition densities used 
to formulate the model. It should be noted that while 
Brownian motion has reversible transition probabilities, the 
OU process does not. How G i ðxÞ is computed will depend 
on the model and its properties; see below for an implemen-
tation of this calculation in the threshold model. 
Once G i ðxÞ has been computed for all nodes, the actual 
(marginal) posterior densities are computed from Bayes' rule. 
Letting u, v be the children of node i, </p>

<p>f ðx i jz 1 ; . . . ; z n Þ ¼ 
G i ðx i ÞF u ðx i ÞF v ðx i Þf ðx i Þ 
LðT Þ 
: 
ð21Þ </p>

<p>Case study: threshold models </p>

<p>In this section, we show how the general framework can be 
applied to the threshold model of Wright (1934) and 
Felsenstein (2005, 2012). Each trait is modeled by a continu-
ously varying "liability" which evolves along branches accord-
ing to a Brownian motion process. While the underlying 
liability is continuous, the observed data are discrete: at each 
tip we observe only whether the liability is above or below 
some threshold. 
We will use standard notation for Gaussian densities. Let 
ðxjm; s 
2 Þ denote the density of a Gaussian random variable x 
with mean and variance s 
2 ; let </p>

<p>Èðyjm; s 
2 Þ ¼ 
Z y </p>

<p>À1 </p>

<p>ðxjm; s 
2 Þ 
ð 22Þ </p>

<p>denote its cumulative density function, with inverse 
È 
À1 ðajm; s 
2 Þ. 
Let X 1 ; . . . ; X 2nÀ1 denote the (unobserved) liability values at 
the n tips and nÀ1 internal nodes. As above we assume that 
the i &lt; j whenever node i is a child of node j, so that the root 
has index 2n À 1. 
The liability value at the root has a Gaussian density with 
mean r and variance s </p>

<p>2 </p>

<p>r : 
f ðx 2nÀ1 jy r Þ ¼ ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ: 
ð23Þ </p>

<p>Consider any nonroot node i and let j be the index of its 
parent. Let t i denote the length of the branch connecting 
nodes i and j. Then X i has a Gaussian density with mean x j 
and variance s 
2 t v : </p>

<p>f ðx i jx j ; y i Þ ¼ ðx i jx j ; s 
2 t i Þ: 
ð24Þ </p>

<p>Following Felsenstein (2005), we assume thresholds for the 
tips are all set at zero. We observe 1 if the liability is positive, 0 if 
the liability is negative, and ? if data are missing. We can in-
clude the threshold step into our earlier framework by defining </p>

<p>pðz i jx i Þ ¼ 
1 if z i ¼ 1 and x i &gt; 0; or z i ¼ 0 and x i 0; or z i ¼ ? </p>

<p>0 
o t h e r w i s e : </p>

<p>( </p>

<p>ð25Þ </p>

<p>The likelihood function for observed discrete values z 1 ; . . . ; 
z n is then given by integrating over liability values for all nodes 
on the tree: </p>

<p>LðT jz 1 ; . . . ; z n Þ ¼ </p>

<p>Z 1 </p>

<p>À1 </p>

<p>Á Á Á </p>

<p>Z 1 </p>

<p>À1 </p>

<p>ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ </p>

<p>Y </p>

<p>ði;jÞ </p>

<p>ðx i jx j ; s 
2 t i Þ 
Y n </p>

<p>i¼1 </p>

<p>pðz i jx i Þ dx 1 . . . dx 2nÀ1 : </p>

<p>ð26Þ </p>

<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>



<p>The first step toward computing LðT jz 1 ; . . . ; z n Þ is to 
bound the domain of integration so that we can apply 
Simpson's method. Ideally, we would like these bounds to 
be as tight as possible, for improved efficiency. For the 
moment we will just outline a general procedure which can 
be adapted to a wide range of evolutionary models. 
The marginal (prior) density of a single liability or trait value 
at a single node is the density for that liability value marginal-
izing over all other values and data. With the threshold model, 
the marginal density for the liability at node i is Gaussian with 
mean r (like the root) and variance v i equal to the sum of the 
variance at the root and the transition variances on the path 
from the root to node i. If P i is the set of nodes from the root to 
node i, then </p>

<p>v i ¼ s </p>

<p>2 </p>

<p>r þ s </p>

<p>2 </p>

<p>X </p>

<p>j2P i </p>

<p>t j : 
ð27Þ </p>

<p>The goal is to constrain the error introduced by truncating 
the integrals with infinite domain. Let be the desired bound 
on this truncation error. Recall that the number of internal 
nodes in the tree is nÀ1. Define </p>

<p>L i ¼ È </p>

<p>À1 </p>

<p>
2ðn À 1Þ 
jm r ; v i </p>



<p>ð28Þ </p>

<p>and </p>

<p>U i ¼ È 
À1 1 À </p>

<p>2ðn À 1Þ 
jm r ; v i </p>



<p>: 
ð29Þ </p>

<p>The bounds L i and U i are chosen so that the (marginal) prob-
ability X i lies outside the interval ½L i ; U i is at most =ðn À 1Þ. 
For this model, these are given by the inverse distribution 
function of a Gaussian; other models would involved different 
transition densities. By the inclusion-exclusion principle, the 
joint probability X i 2 ½L i ; U i for any internal node i is at 
most . We use this fact to bound the contribution of the 
regions outside these bounds. 
Z 1 </p>

<p>À1 </p>

<p>Á Á Á </p>

<p>Z 1 </p>

<p>À1 </p>

<p>f ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ </p>

<p>Y </p>

<p>ðu;vÞ </p>

<p>f ðx v jx u ; y v Þ </p>

<p>Y n </p>

<p>i¼1 </p>

<p>pðz i jx i Þdx 1 . . . dx 2nÀ1 </p>

<p>À </p>

<p>Z b 2nÀ1 </p>

<p>a 2nÀ1 </p>

<p>Á Á Á </p>

<p>Z b nþ1 </p>

<p>a nþ1 </p>

<p>Z 1 </p>

<p>À1 </p>

<p>Á Á Á </p>

<p>Z 1 </p>

<p>À1 </p>

<p>f ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ </p>

<p>Y </p>

<p>ðu;vÞ </p>

<p>f ðx v jx u ; y v Þ 
Y n </p>

<p>i¼1 </p>

<p>pðz i jx i Þdx 1 . . . dx 2nÀ1 </p>

<p>ð30Þ </p>

<p>Z 1 </p>

<p>À1 </p>

<p>Á Á Á </p>

<p>Z 1 </p>

<p>À1 </p>

<p>f ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ </p>

<p>Y </p>

<p>ðu;vÞ </p>

<p>f ðx v jx u ; y v Þdx 1 . . . dx 2nÀ1 </p>

<p>À </p>

<p>Z b 2nÀ1 </p>

<p>a 2nÀ1 </p>

<p>Á Á Á </p>

<p>Z b nþ1 </p>

<p>a nþ1 </p>

<p>Z 1 </p>

<p>À1 </p>

<p>Á Á Á </p>

<p>Z 1 </p>

<p>À1 </p>

<p>f ðx 2nÀ1 jm r ; s </p>

<p>2 </p>

<p>r Þ </p>

<p>Y </p>

<p>ðu;vÞ </p>

<p>f ðx v jx u ; y v Þdx 1 . . . dx 2nÀ1 
ð31Þ </p>

<p>PðX nþ1 = 
2 ½L nþ1 ; U nþ1 or X nþ2 = 
2 ½L nþ2 ; U nþ2 or Á Á Á orX 2nÀ1 
= 
2½L 2nÀ1 ; U 2nÀ1 Þ </p>

<p>ð32Þ </p>

<p>&lt; : 
ð33Þ </p>

<p>We therefore compute values L i , U i for n þ 1 i 2n À 1 
using (28) and (29) repeatedly, and use these bounds when 
carrying out integration at the internal nodes. We define </p>

<p>X i ½k ¼ L i þ 
U i À L i 
N 
k 
ð34Þ </p>

<p>for k ¼ 0; 1; . . . ; N and each internal node i. 
The next step is to use dynamic programming and numer-
ical integration to compute the approximate likelihood. Let 
node i be a tip of the tree, let node j be its parent and let z i 
be the binary trait value at this tip. For each k ¼ 0; 1; . . . ; N 
we use standard error functions to compute </p>

<p>F i ½k ¼ F i ðX j ½kÞ 
ð35Þ </p>

<p>¼ </p>

<p>Z 1 </p>

<p>0 </p>

<p>ðxjX j ½k; s 
2 t i Þdx if z i ¼ 1 </p>

<p>Z 0 </p>

<p>À1 </p>

<p>ðxjX j ½k; s 
2 t i Þdx if z i ¼ 0 </p>

<p>1 
i fz i ¼ ?: </p>

<p>8 
&gt; &gt; &gt; &gt; &gt; &gt; &lt; </p>

<p>&gt; &gt; &gt; &gt; &gt; &gt; : 
ð36Þ </p>

<p>Here, ðxjm; s 
2 Þ is the density of a Gaussian with mean 
and variance s 
2 . 
Now suppose that node i is an internal node with parent 
node j and children u and v. Applying Simpson's rule to the 
bounds L i , U i to (16) we have for each k ¼ 0; 1; . . . ; N: </p>

<p>F i ½k ¼ 
U i À L i 
N </p>

<p>X N </p>

<p>'¼0 </p>

<p>w ' ðX i ½'jX j ½k; s 
2 t i ÞF u ½'F v ½' 
ð37Þ </p>

<p>&amp;F i ðX j ½kÞ: 
ð38Þ </p>

<p>Suppose node i is the root, and u, v are its children. 
Applying Simpson's rule to (17) gives an approximate likeli-
hood of </p>

<p>U 2nÀ1 À L nÀ1 
N </p>

<p>X N </p>

<p>'¼0 </p>

<p>w ' ðX i ½'jm r ; s </p>

<p>2 </p>

<p>r ÞF u ½'F v ½': 
ð39Þ </p>

<p>Hiscott et al. </p>

<p>GBE </p>



<p>Pseudocode for the algorithm appears in Algorithm 1. </p>

<p>Algorithm 1: Compute probability of a threshold character. </p>

<p>Input: 
N : Number of intervals in numerical integration. 
t 1 , . . . , t 2n−2 : branch lengths in tree. 
μ r , σ </p>

<p>2 </p>

<p>r : mean and variance of root density 
σ 
2 : variance of transition densities (per unit branch length) 
z 1 , . . . , z n observed character (z i ∈ {+1, 0, ?}) 
Output: 
Probability L of observed character under the threshold model. </p>

<p>Construct the vector x = [0, 1, 2, . . . , N]/N . 
Construct the vector w = [1, 4, 2, 4, 2, . . . , 4, 2, 1] as in (??) 
Compute the path length p i from the root to each node i. 
Initialize F i [k] ← 1 for all nodes i and 0 ≤ k ≤ N . 
For all i = n + 1, n + 2, . . . , 2n−1 
L i ← Φ 
−1 ( </p>

<p>nN </p>

<p>−4 </p>

<p>2(n−1) |μ r , σ </p>

<p>2 </p>

<p>r + σ 
2 p i ) </p>

<p>U i ← Φ 
−1 (1 − </p>

<p>nN </p>

<p>−4 </p>

<p>2(n−1) |μ r , σ </p>

<p>2 </p>

<p>r + σ 
2 p i ) 
X i ← (U i − L i )x + L i 
For all tip nodes i = 1, 2, . . . , n 
Let j be the index of the parent of node i 
For k = 0, . . . , N 
If z i = 1 
F i [k] = 1 − Φ(0; X j [k], σ 
2 t i ) 
else if z i = 0 
F i [k] = Φ(0; X j [k], σ 
2 t i ) 
For all internal nodes i = n+1, ..., 2n−2, excluding the root 
Let j be the index of the parent of node i 
Let u, v be the indices of the children of node i 
For k = 0, 1, . . . , N </p>

<p>F i [k] ← 
U i − L i 
N </p>

<p>N </p>

<p>=0 </p>

<p>w φ(X i [ ]; X j [k], σ 
2 t i )F u [ ]F v [ ] </p>

<p>Let u, v be indices of the the children of the root. </p>

<p>L ← 
U 2n−1 − L n−1 
N </p>

<p>N </p>

<p>=0 </p>

<p>w φ(X i [ ]; μ r , σ </p>

<p>2 </p>

<p>r )F u [ ]F v [ ] </p>

<p>Algorithm 1 Pseudo-code of the likelihood approximation 
algorithm for a single character, under the threshold model. 
The nodes are numbered in increasing order from tips to the 
root. 
Regarding efficiency and convergence we have: 
Theorem 1 Algorithm 1 runs in OðnN 
2 Þ time and approx-
imates L(T) with OðnN 
À4 Þ error. 
Proof 
The running time follows from the fact that for each of the 
O(n) nodes in the tree we carry out O(N) applications of 
Simpson's method. 
Simpson's rule has OðN 
À4 Þ convergence on functions with 
bounded fourth derivatives (Dahlquist and Bjö rck 2008). The 
root density and each of the transition densities are Gaussians, 
so individually have bounded fourth derivatives. For each node 
i, let n i denote the number of tips which are descendants of 
the node. Using induction on (16), we see that for all nodes i, 
the fourth derivative of F i ðxÞ is Oðn i Þ. 
If we use ¼ nN 
À4 in (28) and (29) then replacing the 
infinite domain integrals with integrals on ½L i ; U i introduces 
at most nN 
À4 error. Using a second induction proof on (16) 
and (37) together with the bound on fourth derivatives, we 
have that jF i ðX j ½kÞ À F i ½kj is at most Oðn i N 
À4 Þ for all nodes i, 
where node j is the parent of node i. In this way we obtain </p>

<p>error bound of Oðn 2nÀ1 N 
À4 Þ ¼ OðnN 
À4 Þ on the approxima-
tion of LðT jz 1 ; . . . ; z n ; yÞ: 
« 
We can estimate posterior densities using the recursion (20) 
followed by equation (21). The conditional density </p>

<p>f ðxjx; y i Þ ¼ xjm r þ 
v j 
v i 
x À m r 
À 
Á 
; 
s 
2 t i v j </p>

<p>v i </p>



<p>ð40Þ </p>

<p>can be obtained by plugging the transitional density </p>

<p>f ðxjx; y i Þ ¼ ðxjx; s 
2 t i Þ 
ð 41Þ </p>

<p>and the two marginal densities (27) </p>

<p>f ðxÞ ¼ ðx; v j Þ; f ðxÞ ¼ ðx; v i Þ 
ð 42Þ </p>

<p>into the identity f ðxjx; y i Þ ¼ f ðxjx; y i Þ </p>

<p>f ðx Þ </p>

<p>f ðxÞ . We thereby obtain 
the recursion </p>

<p>G i ðxÞ ¼ </p>

<p>Z 
xjm r þ 
v j 
v i 
x À m r 
À 
Á 
; 
s 
2 t i v j </p>

<p>v i </p>



<p>G j ðxÞF u ðxÞdx ð43Þ </p>

<p>which we estimate using Simpson's method. Algorithm esti-
mates values of the posterior densities at each node, evaluated 
using the same set of grid points as used in Algorithm 1. An 
additional round of numerical integration can be used to 
obtain posterior means and variances. </p>

<p>Evolutionary Precursors of Plant Extrafloral Nectaries </p>

<p>To study the methods in practice, we reanalyze trait data pub-
lished by Marazzi et al. (2012), using a fixed phylogeny. 
Marazzi et al. (2012) introduce and apply a new discrete 
state model for morphological traits which, in addition to 
states for presence and absence, incorporates an intermediate 
"precursor" state. Whenever the intermediate state is ob-
served at the tips it is coded as "absent." The motivation 
behind the model is that the intermediate state represents 
evolutionary precursors, changes which are necessary for 
the evolution of a new state but which may not be directly 
observed. These precursors could explain repeated parallel 
evolution of a trait in closely related traits (Marazzi et al. 
2012). They compiled a data set recording presence or ab-
sence of plant EFNs across a phylogeny of 839 species of 
Fabales, fitting their models to these data. 
The threshold model also involves evolutionary precursors 
in terms of changes in ancestral liabilities. We use these 
models, and our new algorithms to analyze the EFN data 
set. Our analysis also makes use of the time-calibrated phy-
logeny inferred by Simon et al. (2009), although unlike 
Marazzi et al. (2012) we ignore phylogenetic uncertainty. </p>

<p>Experimental Protocol </p>

<p>We conduct three separate experiments. For the first experi-
ment, we examine the rate of convergence of the likelihood 
algorithm as we increase N. This is done for the "All" EFN 
character (Character 1 in Marazzi et al. [2012]) for a range of </p>

<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>



<p>estimates for the liability variance at the root, s </p>

<p>2 </p>

<p>r . The interest 
in s </p>

<p>2 </p>

<p>r stems from its use in determining bounds L i , U i for each 
node, with the expectation that as s </p>

<p>2 </p>

<p>r increases, the conver-
gence of the integration algorithm will slow. The mean liability 
at the root, r , was determined from the data using Maximum 
Likelihood estimation. 
We also examined convergence of the algorithm on ran-
domly generated characters. We first evolved liabilities accord-
ing to the threshold model, using the parameter settings 
obtained above. To examine the difference in performance 
for non-phylogenetic characters, we also simulated binary 
characters by simulated coin flipping. Twenty replicates 
were carried out for each case. 
The second experiment extends the model comparisons 
carried out in Marazzi et al. (2012) to include the threshold 
models. For this comparison we fix the transitional variance s </p>

<p>2 </p>

<p>at one, since changing this values corresponds to a rescaling of 
the Brownian process, with no change in likelihood. With only 
one character, the maximum likelihood estimate of the root 
variance s </p>

<p>2 </p>

<p>r is zero, irrespective of the data. This leaves a single 
parameter to infer: the value of the liability at the root state. 
We computed a maximum likelihood estimate for the state at 
the root, then applied our algorithm with a sufficiently large 
value of N to be sure of convergence. The Akaike Information 
Criterion (AIC) was determined and compared with those ob-
tained for the model of Marazzi et al. (2012). 
For the third experiment, we determine the marginal pos-
terior densities for the liabilities at internal nodes, using </p>

<p>Algorithm 2. </p>

<p>Algorithm 2: Compute posterior densities </p>

<p>Input: 
N , t 1 , . . . 2n − 2, μ r , σ </p>

<p>2 </p>

<p>r , and σ 
2 as in Algorithm 1 
Vector p, likelihood L and arrays F i computed in Algorithm 1. 
Output: 
Arrays H i for each internal node i. 
Construct the vectors x, w, {L i : i ∈ {n + 1, . . . , 2n − 2}}, 
{U i : i ∈ {n + 1, . . . , 2n − 2}}, and path lengths p i as in Algorithm 1. 
G 2n−1 [k] ← 1 for all k. 
For all i = 2n−2, 2n − 3, . . . , n + 1 
Let j be the index of the parent of node i. 
Let v be the index of the sibling of node i. 
For k = 0, 1, . . . , N 
μ ← μ r + </p>

<p>σ 2 
r +σ 2 pj 
σ 2 </p>

<p>r +σ 2 pi (X i [k] − μ r ) 
V ← </p>

<p>σ 2 ti(σ 2 </p>

<p>r +σ 2 pj ) </p>

<p>σ 2 
r +σ 2 pi </p>

<p>G i [k] ← 
U j − L j 
N </p>

<p>N </p>

<p>=0 </p>

<p>w φ(X j [ ]; μ, V )G j [ ]F v [ ] </p>

<p>For all i = n + 1, . . . , 2n − 1 
Let u, v be the children of node i. 
For all k = 0, 1, . . . , N 
H i [k] ← </p>

<p>1 </p>

<p>L G i [k]F u [k]F v [k]φ(X i [k]|μ r , σ </p>

<p>2 </p>

<p>r + σ 
2 p i ) </p>

<p>Algorithm 2 Pseudocode for the algorithm to efficiently 
compute ancestral posterior densities under the threshold 
model. At the termination of the algorithm, H i ½k is an esti-
mate of the posterior density at internal node i, evaluated at 
x ¼ X i ½k. 
These posterior probabilities are then mapped onto the phy-
logeny, using shading to denote the (marginal) posterior prob-
ability that a liability is larger than zero. We therefore obtain a 
figure analogous to supplementary figure S7, Supplementary 
Material online, of Marazzi et al. (2012). </p>

<p>log(N) </p>

<p>1.2 
1.4 
1.6 
1.8 
2 
2.2 
2.4 
2.6 
2.8 
3 </p>

<p>log(error) </p>

<p>-120 </p>

<p>-100 </p>

<p>-80 </p>

<p>-60 </p>

<p>-40 </p>

<p>-20 </p>

<p>0 </p>

<p>Convergence: Simpson's method </p>

<p>log(N) </p>

<p>1.2 
1.4 
1.6 
1.8 
2 
2.2 
2.4 
2.6 
2.8 
3 </p>

<p>log(error) </p>

<p>-108 </p>

<p>-107.5 </p>

<p>-107 </p>

<p>-106.5 </p>

<p>-106 </p>

<p>-105.5 </p>

<p>-105 </p>

<p>-104.5 </p>

<p>-104 </p>

<p>Convergence: Gaussian kernel method </p>

<p>FIG. 1.-Log-log plots of error as a function of N for the dynamic programming algorithm with Simpson's method (left) and with the Gaussian kernel 
method (right). The likelihoods were computed under the threshold model on EFN trait data for an 839 taxon tree. Dotted lines have slope À4 (corresponding 
to convergence rate of N 
À4 . Note the difference in scale for the two methods.). Logarithms computed to base 10. Letting h be the height of the tree, the </p>

<p>circles in both plots represent errors when s </p>

<p>2 </p>

<p>r ¼ h, the asterisks represent errors when s </p>

<p>2 </p>

<p>r ¼ 0:1h, and the triangles represent errors when s </p>

<p>2 </p>

<p>r ¼ 10h. </p>

<p>Hiscott et al. </p>

<p>GBE </p>



<p>Results </p>

<p>Convergence of the Algorithm </p>

<p>To examine convergence, we compute the absolute error of 
each likelihood approximation because the actual likelihood is 
not available we use the approximation when N = 1,000. Plots 
of error versus N are given in figure 1, both for Simpson's 
method (left) and for the modified Gaussian kernel method 
(right). For larger N, the error in a log-log plot decreases with 
slope at most À4 (as indicated), corresponding to N 
À4 con-
vergence of the method. Log-log plots of error versus N for 
the simulated data are given in figure 2. In each case, the 
method converges for by N&amp;30. 
While the level of convergence for both algorithms is cor-
rect, the accuracy of the method based on Simpson's method 
is far worse. When a branch length is short, the transition 
density becomes highly peaked, as does the function being 
integrated. Such functions are difficult to approximate with 
piecewise quadratics, and Simpson's method can fail misera-
bly. Indeed, for N &lt; 50, we would often observe estimated 
probabilities equal to 0, or estimates greater than 1! (These 
were omitted from the plots). Although we can always bound 
estimates computed by the algorithm, a sounder approach is 
to improve the integration technique. This we did using the 
Gaussian kernel method, and the result was far improved ac-
curacy for little additional computation. For the remainder of 
the experiments with this model we used the Gaussian kernel 
method when carrying out numerical integration. </p>

<p>Table 1 
Table of Log-Likelihood and AIC Values for the Binary Character, 
Precursor, and Threshold Models on Six EFN Traits </p>

<p>Trait 
Model 
k 
log L 
AIC </p>

<p>1 (All) 
Binary 
2 
À251.7 
507.4 
Precursor 
1 
À246.7 
495.4 
Threshold 
1 
À240.6 
483.2 
2 (Leaves) 
Binary 
2 
À240.3 
484.6 
Precursor 
1 
À234.5 
470.9 
Threshold 
1 
À230.6 
463.1 
3 (Inflorescence) 
Binary 
2 
À108.3 
220.5 
Precursor 
1 
À110.9 
223.9 
Threshold 
1 
À108.3 
218.5 
4 (Trichomes) 
Binary 
2 
À86.7 
177.3 
Precursor 
1 
À86.9 
175.9 
Threshold 
1 
À85.8 
173.5 
5 (Substitutive) 
Binary 
2 
À163.0 
330.1 
Precursor 
1 
À161.6 
325.3 
Threshold 
1 
À161.3 
324.6 
6 (True) 
Binary 
2 
À132.3.1 
268.7 
Precursor 
1 
À131.1 
264.3 
Precursor 
2 
À126.7 
257.3 
Threshold 
1 
À125.3 
252.6 </p>

<p>NOTE.-Column k indicates numbers of parameters for each model. Data 
for the binary and precursor models copied from table 1 in Marazzi et al. 
(2012). All likelihoods and AIC values rounded to 1 d.p. Boldface indicates the 
best fitting model for each trait. A pre-cursor model with one parameter was 
used for all experiments, except for trait 6 where a two-parameter model gave 
a better AIC than the one-parameter model (see discussion in Marazzi et al. 
(2012). </p>

<p>log(N) </p>

<p>1 
1.5 
2 
2.5 
3 </p>

<p>logL </p>

<p>-500 </p>

<p>-450 </p>

<p>-400 </p>

<p>-350 </p>

<p>-300 </p>

<p>-250 </p>

<p>-200 </p>

<p>-150 </p>

<p>-100 </p>

<p>-50 </p>

<p>0 </p>

<p>Simulated trait convergence </p>

<p>log(N) </p>

<p>1 
1.5 
2 
2.5 
3 </p>

<p>logL </p>

<p>-760 </p>

<p>-740 </p>

<p>-720 </p>

<p>-700 </p>

<p>-680 </p>

<p>-660 </p>

<p>-640 </p>

<p>-620 </p>

<p>Coin flip trait convergence </p>

<p>FIG. 2.-Plots of log-likelihood values as a function of log ðNÞ for the two types of data simulated from the fixed EFN tree, computed using our algorithm 
together with the Gaussian kernel method. Logarithms computed to base 10. </p>

<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>



<p>Model Comparison </p>

<p>Marazzi et al. (2012) describe AIC comparisons between their 
precursor model and a conventional binary trait model. We 
extend this comparison to include the threshold model. This is 
a one parameter model, the parameter being the value of the 
liability at the root. We used the <rs type="software">MATLAB</rs> command <rs type="software">fmin-
search</rs> with multiple starting points to compute the maximum 
likelihood estimate for this value. The resulting log-likelihood 
was log?L ¼ À240:6, giving an AIC of 483.2. This compares 
to an AIC of 507.4 for the (two parameter) binary character 
model and an AIC of 495.4 for the (one parameter) precursor 
model of Marazzi et al. (2012). 
We analyzed the five other EFN traits in the same way, and 
present the computed AIC values in table 1, together with AIC 
values for the two parameter binary state model and one pa-
rameter precursor model computed by Marazzi et al. (2012) 
(and the two parameter precursor model for trait 6). We see </p>

<p>that the threshold model fits better than either the binary or 
precursor models for all of the six traits. 
It is not clear, a priori, why the threshold model would 
appear to fit some data better than the precursor model be-
cause they appear to capture similar evolutionary phenomena. 
It would be useful to explore this observation more thor-
oughly, given the new computational tools, perhaps incorpo-
rating phylogenetic error in a manner similar to Marazzi et al. 
(2012). </p>

<p>Inferring Ancestral Liabilities </p>

<p>Figure 3 gives a representation of how the (marginal) posterior 
liabilities change over the tree. Branches are divided into three 
classes according to the posterior probability that the liability is 
positive, with lineages with posterior probability &gt; 0.7 colored 
red, lineages with posterior probability &lt; 0.3 colored white, 
and remaining lineages colored pink. </p>

<p>Caesalpinioideae* </p>

<p>Other Fabales </p>

<p>Papilionoideae </p>

<p>Mimosoideae </p>

<p>FIG. 3.-Marginal posterior probabilities for the liabilities, for EFN trait 1 of Marazzi et al. (2012) on the phylogeny inferred by Simon et al. (2009). 
Lineages with posterior probability &gt; 0.7 colored red, lineages with posterior probability &lt; 0.3 colored white, and remaining lineages colored pink. </p>

<p>Hiscott et al. </p>

<p>GBE </p>



<p>This diagram can be compared with Marazzi et al. (2012), 
figure S7. The representations are, on the whole, directly com-
parable. A positive liability corresponds, roughly, to an ances-
tral precursor state. Both analyses suggest multiple origins of a 
precursor state, for example for a large clade of Mimosoidae. 
Interestingly, there are several clades where the analysis of 
Marazzi et al. (2012) suggests widespread ancestral distribu-
tion of the precursor state whereas our analysis indicates a 
negative liability at the same nodes. 
Once again, our analysis is only preliminary, our goal here 
simply being to demonstrate what calculations can now be 
carried out. </p>

<p>Discussion </p>

<p>We have introduced a new framework for the computation 
of likelihoods from continuous characters, and illustrated the 
framework using an efficient algorithm for evaluating (ap-
proximate) likelihoods under Wright and Felsenstein's thresh-
old model. 
This framework opens up possibilities in several directions. 
The numerical integration, or numerical quadrature, literature 
is vast. In this article, we have focused in on a popular and 
simple numerical integration method, and our algorithm 
should be seen as a proof of principle rather than a definitive 
threshold likelihood method. There is no question that the 
numerical efficiency of Algorithm 1 could be improved signif-
icantly through the use of more sophisticated techniques: 
better basis functions or adaptive quadrature methods for a 
start. 
The connection with Felsenstein's (discrete character) prun-
ing algorithm also opens up opportunities for efficiency gains. 
Techniques such as storing partial likelihoods, or approximat-
ing local neighborhoods, are fundamental to efficient phylo-
genetic computations on sequence data (Felsenstein 1981a; 
Larget and Simon 1998; Swofford 2002; Pond and Muse 
2004; Stamatakis 2006). These tricks could all be now applied 
to the calculation of likelihoods from continuous traits. 
Finally, we stress that the algorithm does not depend on 
special characteristics of the continuous trait model, beyond 
conditional independence of separate lineages. Felsenstein's 
pruning algorithm for continuous characters is limited to 
Gaussian processes and breaks down if, for example, the 
transition probabilities are governed by Levy processes 
(Landis et al. 2013). In contrast, our approach works whenever 
we can numerically evaluation transition densities, an indeed 
only a few minor changes would transform our Algorithm 1 to 
one implementing on a far more complex evolutionary 
process. </p>

<p>Supplementary Material </p>

<p>Supplementary material is available at Genome Biology and 
Evolution online (http://www.gbe.oxfordjournals.org/). </p>

<p>Acknowledgments </p>

<p>This research was supported by an Allan Wilson Centre 
Doctoral Scholarship to G.H., financial support to D.B. from 
the Allan Wilson Centre, a Marsden grant to D.B., and finan-
cial support to all authors from the University of Otago. </p>

<p>Literature Cited </p>



<p>Algorithms for Quantitative Trait Models </p>

<p>GBE </p>

<p>Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>





<p>Hiscott et al. </p>

<p>GBE </p>

<p>1350 Genome Biol. Evol. 8(5):1338-1350. doi:10.1093/gbe/evw064 Advance Access publication April 6, 2016 </p>

</text></tei>